{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeGPumfWz4to9aKJL/BhfM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "51c9e199d60e42dba711e7ce0d195101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 2,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".csv",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload Train CSV",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_bb1b478dc42748dbb3bff08cc84933c1",
            "metadata": [
              {
                "name": "training.csv",
                "type": "text/csv",
                "size": 786389,
                "lastModified": 1744587706000
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_a3c987ee681340bb8c6c5ad640b28583"
          }
        },
        "bb1b478dc42748dbb3bff08cc84933c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3c987ee681340bb8c6c5ad640b28583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "b4c13332571b4ea7ba58d513ad9c5069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".csv",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload Test CSV",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_0c91d167ae4c468283f059a185b9eda6",
            "metadata": [
              {
                "name": "testing.csv",
                "type": "text/csv",
                "size": 217991,
                "lastModified": 1744587726000
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_f9ec4e6c451c440cb37683f69674c393"
          }
        },
        "0c91d167ae4c468283f059a185b9eda6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9ec4e6c451c440cb37683f69674c393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "b92d48aff1fc4cacb046afbe3538fb87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".zip",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload Image ZIP",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_3ddb965b3571449b8e7ecd085451bd7e",
            "metadata": [
              {
                "name": "Images.zip",
                "type": "application/zip",
                "size": 211661233,
                "lastModified": 1744588739301
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_d5a315f1cb29467aaa72ef1898e3f7f9"
          }
        },
        "3ddb965b3571449b8e7ecd085451bd7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5a315f1cb29467aaa72ef1898e3f7f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyaprabhakaran7/Code-Mixed-Spanish-VLM-Study/blob/main/CLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP4qaHZ9lrq7",
        "outputId": "2e42e57e-b0cc-42b9-800a-82ad0d720ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tqdm"
      ],
      "metadata": {
        "id": "BCJ-VIFjqjv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0462e978-efb6-40ae-cad1-265def19d3ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# Load CLIP model and processor\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "print(\"✅ CLIP model and processor loaded successfully.\")\n",
        "\n",
        "# Test text and dummy image\n",
        "text = \"A happy tweet\"\n",
        "image = Image.new(\"RGB\", (224, 224), color=\"white\")\n",
        "\n",
        "inputs = processor(text=[text], images=image, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "print(\"✅ Model forward pass completed.\")\n"
      ],
      "metadata": {
        "id": "s1S2dLo-vLaw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8065574e-889a-482b-8a0e-70bbb0a112c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ CLIP model and processor loaded successfully.\n",
            "✅ Model forward pass completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---- Step 1: Manual File Uploads ----\n",
        "print(\"⬆️ Please upload your training CSV, test CSV, and ZIP of images using the upload buttons below.\")\n",
        "\n",
        "import ipywidgets as widgets\n",
        "\n",
        "upload_train_csv = widgets.FileUpload(accept='.csv', multiple=False, description='Upload Train CSV')\n",
        "upload_test_csv = widgets.FileUpload(accept='.csv', multiple=False, description='Upload Test CSV')\n",
        "upload_zip = widgets.FileUpload(accept='.zip', multiple=False, description='Upload Image ZIP')\n",
        "display(upload_train_csv, upload_test_csv, upload_zip)\n"
      ],
      "metadata": {
        "id": "3Bsq1Y-4VH5_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "51c9e199d60e42dba711e7ce0d195101",
            "bb1b478dc42748dbb3bff08cc84933c1",
            "a3c987ee681340bb8c6c5ad640b28583",
            "b4c13332571b4ea7ba58d513ad9c5069",
            "0c91d167ae4c468283f059a185b9eda6",
            "f9ec4e6c451c440cb37683f69674c393",
            "b92d48aff1fc4cacb046afbe3538fb87",
            "3ddb965b3571449b8e7ecd085451bd7e",
            "d5a315f1cb29467aaa72ef1898e3f7f9"
          ]
        },
        "outputId": "a504d1bf-dc5d-47c0-f4b3-fc7932935a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬆️ Please upload your training CSV, test CSV, and ZIP of images using the upload buttons below.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.csv', description='Upload Train CSV')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51c9e199d60e42dba711e7ce0d195101"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.csv', description='Upload Test CSV')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4c13332571b4ea7ba58d513ad9c5069"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.zip', description='Upload Image ZIP')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b92d48aff1fc4cacb046afbe3538fb87"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Read CSV ---\n",
        "def get_dataframe(upload_widget):\n",
        "    for file in upload_widget.value.values():\n",
        "        content = file['content']\n",
        "        return pd.read_csv(BytesIO(content))\n",
        "    return None\n",
        "\n",
        "# --- Extract images from zip ---\n",
        "image_dict = {}\n",
        "\n",
        "def extract_images(upload_widget):\n",
        "    print(\"📦 Extracting images from ZIP...\")\n",
        "    for file in upload_widget.value.values():\n",
        "        zf = zipfile.ZipFile(BytesIO(file['content']))\n",
        "        for full_path in zf.namelist():\n",
        "            if full_path.lower().endswith(('.jpg', '.jpeg', '.png')) and not full_path.endswith('/'):\n",
        "                filename = full_path.split('/')[-1]\n",
        "                with zf.open(full_path) as img_file:\n",
        "                    try:\n",
        "                        image_dict[filename] = Image.open(img_file).convert(\"RGB\")\n",
        "                    except:\n",
        "                        print(f\"❌ Could not load image: {filename}\")\n",
        "    print(f\"✅ Loaded {len(image_dict)} images.\")\n",
        "\n",
        "# --- Load CLIP ---\n",
        "print(\"📥 Loading CLIP model...\")\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "print(\"✅ CLIP model and processor loaded.\")\n"
      ],
      "metadata": {
        "id": "k-JfeSVoWJfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b830017a-f682-4449-f829-298beeed8a36"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Loading CLIP model...\n",
            "✅ CLIP model and processor loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Extract CLIP features ---\n",
        "def extract_features(df, text_col='Original English', file_col='file name'):\n",
        "    features = []\n",
        "    missing = 0\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting features\"):\n",
        "        text = row[text_col]\n",
        "        fname = row[file_col]\n",
        "\n",
        "        # Text embedding\n",
        "        text_inputs = processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        with torch.no_grad():\n",
        "            text_feat = model.get_text_features(**text_inputs)\n",
        "            text_feat /= text_feat.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "        # Image embedding\n",
        "        if fname in image_dict:\n",
        "            image = image_dict[fname]\n",
        "            image_inputs = processor(images=image, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                image_feat = model.get_image_features(**image_inputs)\n",
        "                image_feat /= image_feat.norm(p=2, dim=-1, keepdim=True)\n",
        "        else:\n",
        "            image_feat = torch.zeros_like(text_feat)\n",
        "            missing += 1\n",
        "\n",
        "        combined = torch.cat([text_feat, image_feat], dim=1).squeeze().numpy()\n",
        "        features.append(combined)\n",
        "\n",
        "    if missing > 0:\n",
        "        print(f\"⚠️ {missing} images were missing.\")\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "PS8EsQBRWYh8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run everything\n",
        "train_df = get_dataframe(upload_train_csv)\n",
        "test_df = get_dataframe(upload_test_csv)\n",
        "extract_images(upload_zip)\n",
        "\n",
        "print(\"🧠 Extracting features from training data...\")\n",
        "X_train = extract_features(train_df)\n",
        "y_train = train_df['label'].tolist()\n",
        "\n",
        "print(\"🧪 Extracting features from test data...\")\n",
        "X_test = extract_features(test_df)\n",
        "y_test = test_df['label'].tolist()\n",
        "\n",
        "print(\"🔧 Training classifier...\")\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"📈 Predicting...\")\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"\\n📊 Sentiment Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5CQ4pIZWav3",
        "outputId": "7512ace4-f120-41f8-985f-4c3f8e21a6d9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Extracting images from ZIP...\n",
            "✅ Loaded 0 images.\n",
            "🧠 Extracting features from training data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 3213/3213 [04:22<00:00, 12.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ 3213 images were missing.\n",
            "🧪 Extracting features from test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 804/804 [01:10<00:00, 11.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ 804 images were missing.\n",
            "🔧 Training classifier...\n",
            "📈 Predicting...\n",
            "\n",
            "📊 Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.59      0.72      0.65       268\n",
            "     neutral       0.51      0.37      0.43       268\n",
            "    positive       0.65      0.69      0.67       268\n",
            "\n",
            "    accuracy                           0.59       804\n",
            "   macro avg       0.58      0.59      0.58       804\n",
            "weighted avg       0.58      0.59      0.58       804\n",
            "\n"
          ]
        }
      ]
    }
  ]
}